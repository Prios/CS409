{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sw_lee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import queue\n",
    "import threading\n",
    "import subprocess\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset_1/processed_test.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init files\n",
    "\n",
    "train_file_name = 'train'\n",
    "test_file_name = 'test'\n",
    "\n",
    "full_dir_format = 'dataset_1/{}.txt'\n",
    "processed_file_dir_format = 'dataset_1/processed_{}.txt'\n",
    "\n",
    "train_file_dir = full_dir_format.format(train_file_name)\n",
    "test_file_dir = full_dir_format.format(test_file_name)\n",
    "processed_train_file_dir = processed_file_dir_format.format(train_file_name)\n",
    "processed_test_file_dir = processed_file_dir_format.format(test_file_name)\n",
    "processed_test_file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files\n",
    "\n",
    "train_file = open(train_file_dir, 'r', encoding='utf8')\n",
    "processed_train_file = open(processed_train_file_dir, 'r+', encoding='utf8')\n",
    "\n",
    "test_file = open(test_file_dir, 'r', encoding='utf8')\n",
    "processed_test_file = open(processed_test_file_dir, 'r+', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess files\n",
    "\n",
    "for line in train_file:\n",
    "    processed_train_file.write(line.replace('\\t', '\\s', line.count('\\t') - 1))\n",
    "\n",
    "train = pd.read_csv(processed_train_file_dir, delimiter='\\t', header=None, names=['comment_text', 'type'])\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "for line in test_file:\n",
    "    processed_test_file.write(line.replace('\\t', '\\s', line.count('\\t') - 1))\n",
    "\n",
    "test = pd.read_csv(processed_test_file_dir, delimiter='\\t', header=None, names=['comment_text', 'type'])\n",
    "test = test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat = pd.concat([train, test], axis=0, keys=['train','test'])\n",
    "# concat_test = pd.concat([train[:100], test[:50]], axis=0, keys=['train','test'])\n",
    "\n",
    "# # Change categorical to index codes\n",
    "# concat['type'] = pd.Categorical(concat['type'])\n",
    "# concat_test['type'] = pd.Categorical(concat_test['type'])\n",
    "\n",
    "# # Y is in [0, 1] where 1 is spam and 0 is normal\n",
    "# y_concat = concat['type'].astype('category').cat.codes\n",
    "# y_concat_test = concat_test['type'].astype('category').cat.codes\n",
    "\n",
    "# Change categorical to index codes\n",
    "# train = train[:10000]\n",
    "# test = test[:5000]\n",
    "\n",
    "train['type'] = pd.Categorical(train['type'])\n",
    "test['type'] = pd.Categorical(test['type'])\n",
    "\n",
    "# Y is in [0, 1] where 1 is spam and 0 is normal\n",
    "y_t = train['type'].astype('category').cat.codes\n",
    "y_te = test['type'].astype('category').cat.codes\n",
    "\n",
    "# Get list of comments\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_concat = concat['comment_text']\n",
    "# list_concat_test = concat_test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sw_lee/.pyenv/versions/3.6.2/envs/cs409-3.6.2/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 12 threads\n",
      "execution time: 360.18\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "twitter = Twitter()\n",
    "def thread_pos_tagging(data_list, tag):\n",
    "    q = queue.Queue()\n",
    "    for i, v in enumerate(data_list):\n",
    "        q.put((i, v))\n",
    "        \n",
    "    result = [None] * len(data_list)\n",
    "    def _tagging(result):\n",
    "        while True:\n",
    "            i, v = q.get()\n",
    "            result[i] = \" \".join([\"\".join(w) for w, t in tag.pos(v)])\n",
    "            q.task_done()\n",
    "\n",
    "    cpus=multiprocessing.cpu_count() #detect number of cores\n",
    "    print(\"Creating %d threads\" % cpus)\n",
    "    for i in range(cpus):\n",
    "        t = threading.Thread(target=_tagging, args=(result,))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "\n",
    "    q.join()\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "doc0_t = thread_pos_tagging(list_sentences_train, twitter)\n",
    "print('execution time: %.2f' % (time.time() - start))\n",
    "\n",
    "# doc0_t = [\" \".join([\"\".join(w) for w, t in twitter.pos(s)]) for s in list_sentences_train]\n",
    "# doc0_te = [\" \".join([\"\".join(w) for w, t in twitter.pos(s)]) for s in list_sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc0_te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4d53dd914e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc0_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_tokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc0_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlist_tokenized_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc0_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc0_te' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(doc0_t)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(doc0_t)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(doc0_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_tokenized_train))\n",
    "print(len(list_tokenized_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 26874)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max([len(x) - 1 for x in doc])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize the comments\n",
    "\n",
    "# max_features = 20000\n",
    "# tokenizer = Tokenizer(num_words=max_features, char_level=True)\n",
    "\n",
    "# tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences\n",
    "\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "X_te[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "\n",
    "inp = Input(shape=(maxlen, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed layer\n",
    "\n",
    "embed_size = 128\n",
    "x = Embedding(vocab_size, embed_size)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "\n",
    "x = LSTM(200, return_sequences=False, name='lstm_layer')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Max Pooling layer\n",
    " \n",
    "# x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer (1/2)\n",
    "\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "\n",
    "x = Dense(50, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droupout layer (2/2)\n",
    "\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Dense layer\n",
    "\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 75, 128)           3439872   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 3,713,173\n",
      "Trainable params: 3,713,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "9000/9000 [==============================] - 17s 2ms/step - loss: 0.3046 - acc: 0.8953 - val_loss: 0.0636 - val_acc: 0.9800\n",
      "Epoch 2/5\n",
      "9000/9000 [==============================] - 16s 2ms/step - loss: 0.0240 - acc: 0.9938 - val_loss: 0.0587 - val_acc: 0.9830\n",
      "Epoch 3/5\n",
      "9000/9000 [==============================] - 15s 2ms/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0595 - val_acc: 0.9800\n",
      "Epoch 4/5\n",
      "9000/9000 [==============================] - 15s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0705 - val_acc: 0.9810\n",
      "Epoch 5/5\n",
      "9000/9000 [==============================] - 16s 2ms/step - loss: 4.7848e-04 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1653bc7b8>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "model.fit(X_t, y_t, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 3s 519us/step\n",
      "acc: 68.36%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = model.evaluate(X_te, y_te, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19465198]], dtype=float32)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"못 보신 분들 \\\"마이티빙\\\"에서 무료로 보세요. 가입필요 없음.\"\n",
    "\n",
    "tokenized = tokenizer.texts_to_sequences([test_text])\n",
    "\n",
    "test_sequence = pad_sequences(tokenized, maxlen=maxlen)\n",
    "\n",
    "model.predict(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a():\n",
    "    b = [None] * 10\n",
    "    index = 0\n",
    "    threads = []\n",
    "    def _b(index):\n",
    "        b[index] = index\n",
    "        index += 1\n",
    "    for i in range(10):\n",
    "        threads.append(threading.Thread(target=_b,\n",
    "                                        args=(i,)))\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    return b\n",
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [None] * 10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [None] * 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 12 threads\n",
      "execution time: 0.69\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import threading\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "a = [None] * 100000\n",
    "\n",
    "q = queue.Queue()\n",
    "for i in range(100000): #put 30 tasks in the queue\n",
    "    q.put(i)\n",
    "\n",
    "def worker(a):\n",
    "    while True:\n",
    "        item = q.get()\n",
    "        #execute a task: call a shell program and wait until it completes\n",
    "        a[item] = item\n",
    "        q.task_done()\n",
    "\n",
    "cpus=multiprocessing.cpu_count() #detect number of cores\n",
    "print(\"Creating %d threads\" % cpus)\n",
    "for i in range(cpus):\n",
    "     t = threading.Thread(target=worker, args=(a,))\n",
    "     t.daemon = True\n",
    "     t.start()\n",
    "\n",
    "q.join()\n",
    "print('execution time: %.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 0.01\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "a = [None] * 100000\n",
    "for i in range(100000):\n",
    "    a[i] = i\n",
    "print('execution time: %.2f' % (time.time() - start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
